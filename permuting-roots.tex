\chapter{Permuting roots}
\epigraph[author={Hermann Weyl}, etc={On \'Evariste Galois's letter, written the night before Galois died in a pistol duel. Galois's letter is about permutations of roots of polynomials.}]{This letter, if judged by the novelty and profundity of ideas it contains, is perhaps the most substantial piece of writing in the whole literature of mankind.}\SubIndex{Weyl, Hermann}

\section{Vieta's formulas}

For now, we work over any field.
Which polynomials have which roots?
How are the coefficients related to the roots?
\begin{example}
To get a quadratic polynomial \(p(x)\) to have roots at \(3\) and \(7\), we need it to have \(x-3\) and \(x-7\) as factors.
So it has to be 
\begin{align*}
p(x)
&=
a(x-3)(x-7),
\\
&=
a\pr{x^2-7x-3x+(-3)(-7)},
\\
&=
a\pr{x^2-(3+7)x+3 \cdot 7}.
\end{align*}
\end{example}

A polynomial is \emph{monic}\define{monic polynomial}\define{polynomial!monic} if its leading coefficient is \(1\).
We can make any polynomial monic by dividing off the leading term:
\[
3x^2-4x+1 = 3\pr{x^2-\frac{4}{3}x + \frac{1}{3}}.
\]
The only monic quadratic polynomial \(p(x)\) with roots \(x=3\) and \(x=7\) is
\[
p(x) = x^2-(3+7)x+3 \cdot 7.
\]
The only monic quadratic polynomial \(p(x)\) with roots \(x=r\) and \(x=s\), by the same steps, must be
\[
p(x) = x^2-(r+s)x+rs.
\]
By the same steps, the only cubic polynomial \(q(x)\) with roots at \(x=r\), \(x=s\) and \(x=t\) is
\[
q(x)=(x-r)(x-s)(x-t).
\]
If we multiply it all out, we get
\[
q(x)=x^3-(r+s+t)x^2+(rs+rt+st)x-rst.
\]
Ignoring the minus signs, the coefficients are
\begin{align*}
&1 \\
&r+s+t \\
&rs+rt+st \\
&rst
\end{align*}
There is some pattern: 
\begin{enumerate}
\item
There is a minus sign, turning on and off like a light switch, each time we write down a term.
\item
Each coefficient is a sum of products of roots, taken in all possible ways with a fixed number of roots.
\end{enumerate}

\begin{proposition}[Vieta's formula (Viet\'e)]\SubIndex{Viet\'e, Francois}
If a monic polynomial \(p(x)\) splits into a product of linear factors, say
\[
p(x)=
\pr{x-r_1}
\pr{x-r_2}
\dots
\pr{x-r_n}
\]
then the numbers \(r_1, r_2, \dots, r_n\) are the roots of \(p(x)\), and the coefficients of \(p(x)\), say
\[
p(x) = x^n - a_{n-1} x^{n-1} + \dots \pm  a_1 x \pm a_0,
\]
(with signs switching each term in front of the \(a_j\) coefficients) are computed from the roots by
\begin{align*}
a_{n-1} &= r_1 + r_2 + \dots + r_n, \\
a_{n-2} &= \sum_{i < j} r_i r_j, \\
& \vdotswithin{=} \\
a_i &= \sum_{i_1 < i_2 < \dots < i_{n-i}} r_{i_1} r_{i_2} \dots r_{i_{n-i}}, \\
& \vdotswithin{=} \\
a_2 &= r_1 r_2 \dots r_{n-1} + r_1 r_2 \dots r_{n-2} r_n + \dots + r_2 r_3 \dots r_n, \\
a_1 &= r_1 r_2 \dots r_n.
\end{align*}
\end{proposition}
The \emph{elementary symmetric functions} are the polynomials \(e_1, \dots, e_n\)\Notation{ej}{e_j}{elementary symmetric functions} of variables \(t_1, t_2, \dots, t_n\), given by
\begin{align*}
e_1\of{t_1,t_2,\dots,t_n} &= t_1 + t_2 + \dots + t_n, \\
e_2\of{t_1,t_2,\dots,t_n} &= t_1 t_2 + t_1 t_3 + \dots + t_{n-1} t_n, \\
                          &= \sum_{i < j} t_i t_j, \\
                          & \vdotswithin{=} \\
e_i\of{t_1,t_2,\dots,t_n} &= \sum_{i_1 < i_2 < \dots < i_{n-i}} t_{i_1} t_{i_2} \dots t_{i_{n-i}}, \\
                          & \vdotswithin{=} \\
e_{n-1}\of{t_1,t_2,\dots,t_n} &= t_1 t_2 \dots t_{n-1} + t_1 t_2 \dots t_{n-2} t_n + \dots + t_2 t_3 \dots t_n, \\
e_n\of{t_1,t_2,\dots,t_n} &= t_1 t_2 \dots t_n.
\end{align*}
So we can restate our proposition as:
\begin{proposition}[Vieta's formula]\label{proposition:vieta}
If a monic polynomial \(p(x)\) splits into a product of linear factors, say
\[
p(x)=
\pr{x-r_1}
\pr{x-r_2}
\dots
\pr{x-r_n}
\]
then the numbers \(r_1, r_2, \dots, r_n\) are the roots of \(p(x)\), and the coefficients of \(p(x)\):
\[
p(x) = x^n - e_1 x^{n-1} + \dots (-1)^{n-1}  e_{n-1} x + (-1)^n e_n,
\]
are the elementary symmetric functions 
\[
e_i=e_i\of{r_1,r_2,\dots,r_n}
\]
of the roots \(r_1, r_2, \dots, r_n\).
\end{proposition}

\begin{proof}
We can see this immediately for linear polynomials: \(p(x)=x-r\), and we checked it above for quadratic and cubic ones.
It is convenient to rewrite the elementary symmetric functions as 
\[
e_j\of{r_1,r_2,\dots,r_n}
=
\sum r_1^{b_1} r_2^{b_2} \dots r_n^{b_n}
\]
where the sum is over all choices of numbers \(b_1, b_2, \dots, b_n\) so that
\begin{enumerate}
\item
each of these \(b_i\) is either \(0\) or \(1\) and
\item
so that altogether 
\[
b_1 + b_2 + b_3 + \dots + b_n = j,
\]
\end{enumerate}
or in other words, ``turn on'' \(j\) of the roots, and turn off the rest, and multiply out the ones that are turned on, and then sum over all choices of which to turn on.

If we expand out the product
\[
p(x)=
\pr{x-r_1}
\pr{x-r_2}
\dots
\pr{x-r_n}
\]
we do so by pick whether to multiply with \(x\) or with \(-r_1\) from the first factor, and then whether to multiply with \(x\) or with \(-r_2\) from the second, and so on, and we add over all of these choices.
Each choice write as \(b_1=0\) if we pick to multiply in the \(x\), but \(b_1=1\) if we pick to multiply in the \(-r_1\), and so on:
\begin{align*}
p(x) 
&= 
\sum_{b_1,\dots,b_n} \pr{-r_1}^{b_1} \pr{-r_2}^{b_2} \dots \pr{-r_n}^{b_n}
x^{\pr{1-b_1} + \pr{1-b_2} + \dots + \pr{1-b_n}},
\\
&=
\sum_j
\sum_{b_1+\dots+b_n=j} \pr{-r_1}^{b_1} \pr{-r_2}^{b_2} \dots \pr{-r_n}^{b_n}
x^{n-j}.
\end{align*}
%
%
%We can write the result we want to prove as
%\[
%p(x)\equalquestion \sum (-1)^j
%r_1^{b_1} 
%r_2^{b_2}
%\dots
%r_n^{b_n}
%x^{n-j},
%\]
%where the sum is over \(0 \le j \le n\) and over \(b_i=0\) or \(1\) with
%\[
%j=b_1+b_2+\dots+b_n.
%\]
%But then we divide each root by \(x\) and write this as:
%\[
%p(x)\equalquestion x^n \sum 
%\pr{-\frac{r_1}{x}}^{b_1} 
%\pr{-\frac{r_2}{x}}^{b_2}
%\dots
%\pr{-\frac{r_n}{x}}^{b_n},
%\]
%where the sum is now over all choices of \(b_i=0\) or \(1\).
%By induction, suppose we have this result proven for all polynomials of degree less than \(n\).
%Factor \(p(x)=q(x)\pr{x-r_n}\).
%Assume the result for \(q(x)\):
%\[
%q(x)= x^{n-1} \sum 
%\pr{-\frac{r_1}{x}}^{b_1} 
%\pr{-\frac{r_2}{x}}^{b_2}
%\dots
%\pr{-\frac{r_{n-1}}{x}}^{b_{n-1}},
%\]
%and multiply out to find
%\[
%p(x)=x^n 
%\sum 
%\pr{-\frac{r_1}{x}}^{b_1} 
%\pr{-\frac{r_2}{x}}^{b_2}
%\dots
%\pr{-\frac{r_{n-1}}{x}}^{b_{n-1}}
%\pr{1-\frac{r_n}{x}}.
%\]
%The last factor splits into terms that don't include \(r_n\), and then those that do, corresponding to adding up \(b_1, b_2, \dots, b_n\) with \(b_n=0\) and then with \(b_n=1\).
\end{proof}

\section{Symmetric functions}
A function \(f\left(t_1,t_2,\dots,t_n\right)\) is \emph{symmetric}\define{symmetric!function}
if its value is unchanged by permuting the variables
\(t_1, t_2, \dots, t_n\).

For example, \(t_1+t_2+\dots+t_n\) is clearly symmetric.
For any numbers \(t=\pr{t_1, t_2, \dots, t_n}\)
let
\[
P_t(x)\defeq\left(x-t_1\right)\left(x-t_2\right) \dots \left(x-t_n\right).
\]
Clearly the roots of \(P_t(x)\) are precisely the entries of the vector
\(t\).

Let \(e(t)\defeq\pr{e_1(t),e_2(t),\dots,e_n(t)}\), so that \(e\) maps vectors to vectors (with entries in our field).

\begin{lemma}
Over the field of complex numbers, the map \(e\) is onto, i.e. for each complex vector \(c\) there is a complex vector \(t\) so that \(e(t)=c\).
In particular, there is no relation between the elementary symmetric functions.
\end{lemma}
\begin{proof}
Let \(t_1, t_2, \dots, t_n\) be the complex roots of the polynomial
\[
P(x) = x^n - c_1 x^{n-1} + c_2 x^{n-2} + \dots + (-1)^n c_n.
\]
Such roots exist by the fundamental theorem of algebra (see
theorem~\vref{theorem:FTA}). 
By proposition~\vref{proposition:vieta}, \(P_t(x)\) has coefficients precisely the same as those of \(P(x)\), i.e. \(P_t(x)=P(x)\).
\end{proof}
\begin{lemma}
Over any field, the entries of two vectors \(s\) and \(t\) are permutations of one another just when \(e(s)=e(t)\), i.e. just when the elementary symmetric functions take the same values at \(s\) and at \(t\).
\end{lemma}
\begin{proof}
The roots of \(P_s(x)\) and \(P_t(x)\) are the same numbers.
\end{proof}
\begin{corollary}
A function is symmetric just when it is a function of the elementary symmetric functions.
\end{corollary}
This means that every symmetric function \(f\of{t_1,t_2,\dots,t_n}\) has
the form \(f(t)=h(e(t))\), for a unique function \(h\),
and conversely if \(h\) is any function at all, then \(f(t)=h(e(t))\)
determines a symmetric function.
We want a recipe to write down each symmetric polynomial function in terms of the elemenary symmetric functions, to find this mysterious \(h\).

\begin{example}
Take the function 
\[
f=3x^2yz+3xy^2z+3xyz^2+5xy+5xz+5yz.
\]
Clearly the terms with the 5's look like an elementary symmetric function:
\[
f=3x^2yz+3xy^2z+3xyz^2+5e_2.
\]
(We write \(e_2\) to mean the function \(e_2(x,y,z)=xy+xz+yz\), the 2nd elementary symmetric function.)
But what about the terms with the 3's?
Factor them all together as much as we can:
\[
f=3xyz(x+y+z)+5e_2.
\]
Then it is clear:
\[
f=3e_3e_1+5e_2.
\]
\end{example}

If \(a=\pr{a_1,a_2,\dots,a_n}\), write \(t^a\)\Notation{xa}{x^a}{multivariable exponent} to mean
\(t_1^{a_1} t_2^{a_2} \dots t_n^{a_n}\). 
Order terms by ``alphabetical'' order, also called \emph{weight}\define{weight}: order monomials by the order in \(t_1\); if two monomials have the same order in \(t_1\), break the tie by looking at the order in \(t_2\), and so on.
\begin{example}
The term
\[
t_1^5 t_2^3 
\]
has higher weight than any of
\[
t_1^3 t_2^5, t_1^5 t_2^2, 1, \text{ and } t_2^{1000}.
\]
\end{example}
\begin{example}
Showing the highest order term, and writing dots to indicate lower order terms,
\begin{align*}
e_1(t)&=t_1 + \dots, \\
e_2(t)&=t_1t_2 + \dots, \\
& \vdotswithin{=} \\
e_j(t)&=t_1 t_2 \dots t_j + \dots \\
& \vdotswithin{=} \\
e_n(t)&=t_1 t_2 \dots t_n.
\end{align*}
\end{example}

If a symmetric polynomial contains a term, then it also contains every term obtained by permuting the variables.
The weights appear in all possible permutations, like
\[
t_1^3 t_2^5 + t_1^5 t_2^3.
\]
Among those permutations, the unique highest weight term, say \(t^a\), has 
\[
a=\pr{a_1,a_2,\dots,a_n}
\]
with \(a_1 \ge a_2 \ge \dots \ge a_n\).

\begin{example}
Take a symmetric polynomial 
\[
f=6 t_1^{16} t_2^9 t_3^7 + \dots
\]
where we only write out the highest weight term.
The highest weight term contains the variables \(t_1, t_2, t_3\), with smallest power in \(t_3\) and higher powers in the others.
Inside the highest weight term, each of \(t_1, t_2, t_3\) appears to at least a power of \(7\).
Factor out \(7\) powers of each variable; we underline that factored out part so you can see it:
\[
f=6 t_1^9 t_2^2 \underline{\pr{t_1 t_2 t_3}^7} + \dots
\]
In the remaining factors, each variable appears at least to a power of \(2\), so we factor out 2 of each:
\[
f=6 t_1^7 \underline{\pr{t_1 t_2}^2} \pr{t_1 t_2 t_3}^7 + \dots
\]
So finally it is clear that \(f\) has the same highest weight as
\(
6 e_1^7 e_2^2 e_3^7.
\)
Hence 
\[
f = 6 e_1^7 e_2^2 e_3^7 + \dots
\]
up to terms of lower weight.
\end{example}


If we pick integers \(d_1, d_2, \dots, d_n \ge 0\),
\begin{align*}
e_1(t)^{d_1} e_2(t)^{d_2} \dots e_n(t)^{d_n}
&=
t_1^{d_1} \pr{t_1t_2}^{d_2} \dots \pr{t_1t_2\dots t_n}^{d_n} + \dots,
\\
&=
t_1^{d_1+d_2+\dots+d_n} t_2^{d_2+d_3+\dots+d_n} \dots 
t_n^{d_n} + \dots.
\end{align*}
Write this more neatly as
\[
e(t)^d = t^a+\dots
\]
where
\begin{align*}
a_1&\defeq d_1+d_2+d_3+\dots+d_n, \\ 
a_2&\defeq d_2+d_3+\dots+d_n, \\
a_3&\defeq d_3+\dots+d_n
& \vdotswithin{\defeq} \\
a_n&\defeq d_n.
\end{align*}

On the other hand, given a decreasing sequence of nonnegative integers 
\[
a=\pr{a_1,a_2,\dots,a_n}
\]
calculate \(d\) by
\begin{align*}
d_1&\defeq a_2-a_1, \\
d_2&\defeq a_3-a_2, \\
& \vdotswithin{\defeq} \\
d_n&\defeq a_n
\end{align*}
and then \(e(t)^d=t^a+\dots\).
So for each decreasing sequence \(a=\pr{a_1,a_2,\dots,a_n}\), we have \(t^a=e(t)^d+\dots\).

\begin{theorem}
Every symmetric polynomial \(f\) has exactly one expression as a polynomial
in the elementary symmetric polynomials.
If \(f\) has real/rational/integer coefficients,
then \(f\) is a real/rational/integer coefficient
polynomial of the elementary symmetric polynomials.
\end{theorem}
\begin{proof}
The highest weight term in \(f\) is a constant multiple of a monomial \(t^a\).
Every symmetric polynomial, if it contains a monomial \(t^a\) among its terms, also contains \(t^b\), for any \(b\) obtained by permuting the entries of \(a\). 
So if there is any term at all, there is one \(t^a\) for which \(a\) has decreasing entries.
As above, let
\[
d_n\defeq a_n, d_{n-1}\defeq a_{n-1}-a_n, d_{n-2}\defeq a_{n-2}-a_{n-1}, \dots, d_1\defeq a_1-a_2.
\]
Then \(e(t)^d\) has leading term \(t^a\), so subtracting off a suitable multiple of \(e(t)^d\) from \(f\) gives us a polynomial whose highest weight term is lower than \(t^a\).
Apply induction on the highest term.
\end{proof}
\begin{example}
The sum of squares of two variables is symmetric: 
\[
t_1^2+t_2^2=\pr{t_1+t_2}^2-2 \, t_1 t_2.
\] 
To compute out these expressions: \(f(t)=t_1^2+t_2^2\) has highest term \(t_1^2\). 
The polynomials \(e_1(t)\) and \(e_2(t)\) have highest terms \(t_1\) and \(t_1t_2\).
So we subtract off \(e_1(t)^2\) from \(f(t)\), and find \(f(t)-e_1(t)^2=-2 \, t_1 t_2=-2 e_2(t)\).
\end{example}

\begin{problem}{permuting.roots:express.in.s}
Express each of the following polynomials as polynomials in the elementary symmetric functions:
\begin{enumerate}
\item \(4xyz^3+4xzy^3+4yzx^3\)
\item \(x^4y^4z^4\)
\item \(xyz+x^2y^2z^2\)
\item \(x^3y^3z + x^3yz^z+xy^3z^3\)
\end{enumerate}
\end{problem}
\begin{answer}{permuting.roots:express.in.s}
\begin{enumerate}
\item \(4xyz^3+4xzy^3+4yzx^3=4e_3(e_1^2-2e_2)\)
\item \(x^4y^4z^4=e_3^4\)
\item \(xyz+x^2y^2z^2=e_3\pr{1+e_3}\)
\item \(x^3y^3z + x^3yz^z+xy^3z^3=e_3\pr{e_2^2-2e_3e_1}\)
\end{enumerate}
\end{answer}


\section{Sage}

Sage can compute with elementary symmetric polynomials.
It writes them in a strange notation.
The polynomials we have denoted by \(e_3\) sage denotes by \verb!e[3]!.
More strangely, sage denotes \(e_1 e_2^3 e_5^2\) as \verb!e[5,5,2,2,2,1]! which is the same in sage as \verb!e[2]^3*e[1]*e[5]^2!.
You can't write \verb!e[1,2,2,2,5,5]!; you have to write it as \verb!e[5,5,2,2,2,1]!: the indices have to decrease.
Set up the required rings:
\begin{sageblock}
P.<w,x,y,z>=PolynomialRing(QQ)
S=SymmetricFunctions(QQ)
e=S.e()
\end{sageblock}
This creates a ring \(P=\Q{}[w,x,y,z]\), and then lets \(S\) be the ring of symmetric functions.
The last (mysterious) line sets up the object \(e\) to be the elementary symmetric functions of the ring \(S\).
We can then define a polynomial in our variables:
\begin{sageblock}
f = w^2+x^2+y^2+z^2+w*x+w*y+w*z+x*y+x*z+y*z
e.from_polynomial(f)
\end{sageblock}
which prints out \(\sage{e.from_polynomial(f)}\), the expression of \(f\) in terms of elementary symmetric functions.
To expand out a symmetric polynomial into \(w,x,y,z\) variables:
\begin{sageblock}
q = e[2,1]+e[3]
q.expand(4,alphabet=['w','x','y','z'])
\end{sageblock}
prints out 
\[
\sage{q.expand(4,alphabet=['w','x','y','z'])}
\]


\section{Sums of powers}

Define \(p_j(t)=t_1^j+t_2^j+\dots+t_n^j\),\Notation{pj}{p_j}{sum of powers polynomials} the sums of powers.
\begin{lemma}[Isaac Newton\SubIndex{Newton, Isaac}]
The sums of powers are related to the elementary symmetric functions by
\begin{align*}
0 &= e_1 - p_1, \\
0 &= 2 \, e_2 - p_1 \, e_1 + p_2, \\ 
& \vdotswithin{=} \\
0 &= k \, e_k - p_1 \, e_{k-1} + p_2 \, e_{k-2} - \dots + (-1)^{k-1} p_{k-1} \, e_1
+ (-1)^k p_k,
\end{align*}
\end{lemma}
Using these equations, we can write the elementary symmetric functions inductively in terms of the sums of powers, or vice versa.
\begin{proof}
Let's write \(t^{({\ell})}\) for \(t\) with the \({\ell}\)\textsuperscript{th} entry removed, so if \(t\) is a vector with \(n\) entries, then \(t^{({\ell})}\) is a vector with \(n-1\) entries.
\begin{align*}
p_j e_{k-j}
&=
\sum_{\ell} t_{\ell}^j
\sum_{i_1 < i_2 < \dots < i_{k-j}} t_{i_1} t_{i_2} \dots t_{i_{k-j}}
\\
\intertext{Either we can't pull a $t_{\ell}$ factor out of the second sum, or we can:}
&=
\sum_{\ell} t_{\ell}^j
\sum^{i_1, i_2, \dots \ne {\ell}}_{i_1 < i_2 < \dots < i_{k-j}}
t_{i_1} t_{i_2} \dots t_{i_{k-j}}
+
\sum_{\ell} t_{\ell}^{j+1}
\sum^{i_1, i_2, \dots \ne {\ell}}_{i_1 < i_2 < \dots < i_{k-j-1}}
t_{i_1} t_{i_2} \dots t_{i_{k-j-1}}
\\
&=
\sum_{\ell} t_{\ell}^j e_{k-j}\left(t^{({\ell})}\right)
+
\sum_{\ell} t_{\ell}^{j+1} e_{k-j-1}\left(t^{({\ell})}\right).
\end{align*}
Putting in successive terms of our sum,
\begin{align*}
p_j e_{k-j} - p_{j+1} e_{k-j-1}
=&
\sum_{\ell} t_{\ell}^j e_{k-j}\left(t^{(\ell)}\right)
+
\sum_{\ell} t_{\ell}^{j+1} e_{k-j-1}\left(t^{({\ell})}\right)
\\
-&
\sum_{\ell} t_{\ell}^{j+1} e_{k-j-1}\left(t^{({\ell})}\right)
-
\sum_{\ell} t_{\ell}^{j+2} e_{k-j-2}\left(t^{({\ell})}\right)
\\
=&
\sum_{\ell} t_{\ell}^j e_{k-j}\left(t^{({\ell})}\right)
-
\sum_{\ell} t_{\ell}^{j+2} e_{k-j-2}\left(t^{({\ell})}\right).
\end{align*}
Hence the sum collapses to
\begin{align*}
p_1 e_k - p_2 e_{k-1} + \dots + (-1)^{k-1} p_{k-1} e_1
&=
\sum_{\ell} t_{\ell} e_{k-1}\left(t^{({\ell})}\right)
+(-1)^{k-1}
\sum_{\ell} t_{\ell}^k \cdot e_0\left(t^{(\ell)}\right)
\\
&=
k \, e_k + (-1)^{k-1} p_k.
\end{align*}
\end{proof}
\begin{proposition}
Every symmetric polynomial is a polynomial in the sums of powers. If the coefficients of the symmetric polynomial are real (or rational), then it is a real (or rational) polynomial function of the sums of powers. 
\end{proposition}
\begin{proof}
We can solve recursively for the sums of powers in terms of the elementary symmetric functions and conversely.
\end{proof}

\section{The invariants of a square matrix}
Over any field, a polynomial \(f(A)\) in the entries of a square matrix \(A\), with coefficients in the field, is \emph{invariant}\define{invariant!of square matrix} if \(f\left(FAF^{-1}\right)=f(A)\) for any invertible matrix \(F\) with coefficients in the field.
An invariant is independent of change of basis: if \(T \colon V \to V\) is a linear map on an \(n\)-dimensional vector space, we can define the value \(f(T)\) of any invariant \(f\) of \(n \times n\) matrices, by letting \(f(T)\defeq f(A)\) where \(A\) is the matrix associated to \(T\) in some basis of \(V\).
\begin{example}
For any \(n \times n\) matrix \(A\), write
\[
\det(A-\lambda I)=\chi_A(\lambda)=
e_n(A) - e_{n-1}(A) \lambda + e_{n-2}(A) \lambda^2 + \dots + (-1)^n \lambda^n.
\]
The functions \(e_1(A), e_2(A), \dots, e_n(A)\) are invariants, while \(\chi_A(\lambda)\) is the \emph{characteristic polynomial}\define{characteristic polynomial} of \(A\).
\end{example}
\begin{example}\label{example:Powers}
If we write the trace of a matrix \(A\) as \(\tr{A}\), then the functions
\[
p_k(A) = \tr\of{A^k}.\SubIndex{trace}
\]
are invariants.
\end{example}
\begin{problem}{SymmetricFunctions:Two}
If \(A\) is diagonal, say
\[
A =
\begin{pmatrix}
t_1 \\
& t_2 \\
& & \ddots \\
& & & t_n
\end{pmatrix},
\]
then prove that \(e_j(A)=e_j\left(t_1,t_2,\dots,t_n\right)\), the elementary symmetric functions of the eigenvalues.
\end{problem}
\begin{answer}{SymmetricFunctions:Two}
\begin{align*}
\chi_A{\lambda}
&=
\left(t_1-\lambda\right)\left(t_2-\lambda\right)
\dots \left(t_n-\lambda\right)\\
&=(-1)^n P_{t}\left(\lambda\right)\\
&=
e_n(t) - e_{n-1}(t) \lambda + e_{n-2}(t) \lambda^2 + \dots + (-1)^n \lambda^n.
\end{align*}
\end{answer}
\begin{problem}{permuting.roots:diag.symmetric}
Generalize the previous exercise to \(A\) diagonalizable.
\end{problem}
\begin{problem}{permuting.roots:inverse.Cramer}
Prove that the entries of \(A^{-1}\) are rational functions of the entries of the square matrix \(A\), over any field.
\end{problem}
\begin{answer}{permuting.roots:inverse.Cramer}
Cramer's rule says that the inverse \(A^{-1}\) has entries
\[
A^{-1}_{ij} = \frac{(-1)^{i+j}\det (A \text{ with row \(j\) and column \(i\) deleted})}{\det A},
\]
so rational functions of the entries of \(A\).
\end{answer}

\begin{example}
Let \(\Delta_A\) be the discriminant of the characteristic polynomial of \(A\): \(\Delta_A \defeq \Delta_{\chi_A}\).
The map \(A \mapsto \Delta_A\) is also a polynomial invariant of \(A\), as the coefficients of the characteristic polynomial are.
\end{example}
\begin{problem}{SymmetricFunctions:discriminant}
Take a square matrix \(A\).
Suppose that the characteristic polynomial of \(A\) splits into linear factors.
Prove that \(\Delta_A = 0\) just when \(A\) has an eigenvalue of multiplicity two or more.
\end{problem}
\begin{theorem}
Every invariant polynomial of square matrices over an infinite field has exactly one expression as a polynomial function of the elementary symmetric functions of the eigenvalues. 
\end{theorem}
We can replace the elementary symmetric functions of the eigenvalues by the sums of powers of the eigenvalues.
\begin{proof}
Take an invariant polynomial \(f(A)\).
Every invariant polynomial \(f(A)\) determines an invariant polynomial \(f(t)\)
by setting
\[
A =
\begin{pmatrix}
t_1 \\
& t_2 \\
& & \ddots \\
& & & t_n
\end{pmatrix}.
\]
Taking \(F\) any permutation matrix, invariance tells us that \(f\left(FAF^{-1}\right)=f(A)\). 
But \(f\left(FAF^{-1}\right)\) is given by applying the associated permutation to the entries of \(t\).
Therefore \(f(t)\) is a symmetric function. 
Therefore \(f(t)=h(e(t))\), for some polynomial \(h\); so \(f(A)=h(e(A))\) for  diagonal matrices. 
Replace \(f\) by \(f(A)-h(e(A))\) to arrange that \(f(A)=0\) on all diagonal matrices \(A\).
By invariance, \(f(A)=0\) on all diagonalizable matrices. 

The equation \(f(FAF^{-1})=f(A)\) holds for all \(F\) over our field.
Imagine that \(F\) has abstract variables as entries.
The difference \(f(FAF^{-1})-f(A)\) is a rational function in the entries of \(F\).
Its numerator vanishes for any choice of values of those abstract variables, and so vanishes.
Hence \(f(FAF^{-1})=f(A)\) for \(F\) with abstract variable entries.
In particular, if our field lies in a larger field, then \(f\) remains invariant over the larger field, because we can plug into the entries of \(F\) the values in the larger field.

Recall that a matrix has an eigenvector for each eigenvalue, so if there are \(n\) distinct eigenvalues, then \(A\) is diagonalizable.
This occurs just when the characteristic polynomial \(\chi_A(\lambda)\) splits into distinct linear factors, and so \(\Delta_A \ne 0\).
Conversely, if \(\Delta_A \ne 0\) and \(\chi_A(\lambda)\) splits into linear factors, then they are distinct, and so \(f(A)=0\).
We will see (in theorem~\vref{theorem:splitting.field}) that we can sit our field into a larger field in which the characteristic polynomial \(\chi_A(\lambda)\) splits into linear factors, and therefore if \(\Delta_A\ne 0\) then \(f(A)=0\).

Pick any matrix \(A_0\) whose eigenvalues are all distinct.
In particular, \(\Delta_{A_0} \ne 0\).
Take any matrix \(A_1\).
For an abstract variable \(t\), let \(A_t \defeq (1-t)A_0+tA_1\).
Since \(\Delta_{A_t}\) is a polynomial, not vanishing at \(t=0\), it is a nonzero polynomial in \(t\).
So \(\Delta_{A_t}\ne 0\) except for finitely many \(t\).
Hence \(f(A_t)=0\) except for finitely many \(t\).
But \(f(A)\) is a polynomial, so vanishes for all \(t\).
\end{proof}

\begin{example}
The function \(f(A)=e_j\left(\left|\lambda_1\right|,\left|\lambda_2\right|,\dots,\left|\lambda_n\right|\right)\), where \(A\) has eigenvalues \(\lambda_1, \lambda_2, \dots, \lambda_n\), is a continuous invariant function of a real matrix \(A\), and is \emph{not} a polynomial in \(\lambda_1, \lambda_2, \dots, \lambda_n\).
\end{example}







\section{Resultants and permutations}

A polynomial is \emph{homogeneous}\define{homogeneous!polynomial}\define{polynomial!homogeneous} if all of its terms have the same total degree, i.e. sum of degrees in all variables.

\begin{problem}{permuting:homog.test}
Prove that a polynomial \(b(x)\) of degree \(m\) in some variables \(x_1,x_2,\dots,x_n\) over an infinite field is homogeneous just when \(b(tx)=t^mb(x)\) for every \(t\).
Give a counterexample over a finite field.
\end{problem}

\begin{lemma}
Over any field, every factor of a homogeneous polynomial is homogeneous.
\end{lemma}
\begin{proof}
Write the polynomial factorized, as
\(
b(x)c(x)
\),
and then expand out \(b(x)\) and \(c(x)\) into sums of terms of various orders.
Then \(b(x)c(x)\) expands into products of such terms, with orders adding up, and they must all add up to the same total, so the highest total degree terms multiply together and the lowest total degree terms multiply together both give the same total degree in the product.
\end{proof}

In chapter~\ref{chapter:resultants}, we saw that for any two polynomials split into linear factors
\begin{align*}
b(x) &= \pr{x-\beta_1} \pr{x-\beta_2} \dots \pr{x-\beta_m}, \\
c(x) &= \pr{x-\gamma_1} \pr{x-\gamma_2} \dots \pr{x-\gamma_n}, \\
\end{align*}
the resultant is
\begin{align*}
\resultant{b}{c} 
=
\pr{\beta_1-\gamma_1}\pr{\beta_1-\gamma_2}\dots\pr{\beta_m-\gamma_n}.
\end{align*}
So the resultant is homogeneous of degree \(m+n\) in the variables \(\beta_i, \gamma_j\).
The resultant is clearly invariant under any permutation of the roots of \(b(x)\), and also under any permutation of the roots of \(c(x)\).
Indeed the resultant is by definition expressed in terms of the coefficients of \(b(x)\) and \(c(x)\), not the roots.
The coefficients are homogeneous polynomials in the roots, elementary symmetric functions.
Expanding out the coefficients
\begin{align*}
b(x) &= x^m + b_{m-1} x^{m-1} + \dots + b_0, \\
     &= x^m - e_1(\beta) x^{m-1} + e_2(\beta)x^{m-2} + \dots \pm e_m(\beta),
     \\
c(x) &= x^n + c_{n-1} x^{n-1} + \dots + c_0, \\
     &= x^n - e_1(\gamma) x^{n-1} + e_2(\gamma)x^{n-2} + \dots \pm e_n(\gamma),
\end{align*}
we see that \(b_j\) has degree \(m-j\) in the roots.

Suppose now that we just take any polynomial \(b(x)\) of degree \(m\) with coefficients being abstract variables \(b_j\). 
We will now invent a different concept of \emph{weight}.
It seems natural to assign each coefficient \(b_j\) the \emph{weight}\define{weight} \(m-j\), and then define the \emph{weight} of a monomial in the \(b_j\) to be  the sum of weights of its factors.
If \(b(x)\) splits, the weight of a polynomial in the coefficients of \(b(x)\) is precisely its degree as a polynomial in the roots of \(b(x)\).
In particular, the weight of the resultant is \(mn\), when \(b_i\) receives weight \(m-i\) and \(c_j\) receives weight \(n-j\).
For example, from our examples we computed, we can see that there is a term \(b_0^n=b_{m-m}^n c_{n-0}^0\), and by symmetry there is a term \((-1)^{mn} c_0^m = (-1)^{mn} b_{m-m}^0 c_{n-n}^m\), also of weight \(mn\).

\begin{proposition}\label{proposition:resultant.degree}
Given any two polynomials \(b(x,y)\) of total degree \(m\) and \(c(x,y)\) of total degree \(m\) over a field, either there are at most \(mn\) values of \(y\) for which there is some point \((x,y)\) which satisfies both \(b(x,y)=0\) and \(c(x,y)=0\), or \(b(x,y)\) and \(c(x,y)\) have a common factor as polynomials in \(x,y\).
If the polynomials are homogeneous then either they have a homogeneous common factor, or their resultant is homogeneous nonzero of degree \(mn\).
\end{proposition}
\begin{proof}
Thinking of the polynomials as polynomials in \(x\) with coefficients rational in \(y\), we look at the resultant.
Expanding out 
\[
b(x,y)=\sum_{j+k \le m} b_{jk} x^k y^j=\sum_j b_j(y) x^j,
\]
we see that \(b_j(y)\) has degree at most \(m-j\), exactly the weight of the coefficient \(b_j(y)\) as it enters into the resultant.
Therefore the resultant is a polynomial of degree at most \(mn\) in \(y\).
The resultant vanishes at those values \(y=a\) for which there is a common factor between \(b(x,a)\) and \(c(x,a)\), and in particular if there is a common root it vanishes.
So there are at most \(mn\) such points or the resultant is everywhere zero.
If the resultant vanishes everywhere, then \(b(x,y)\) and \(c(x,y)\) have a common factor with coefficients rational functions of \(y\).
By Gauss's lemmma (proposition~\vref{proposition:Gauss.lemma}) they also have a common factor in polynomials in \(x,y\). 

For homogeneous polynomials, each term \(b_j(y)\) has degree exactly \(m-j\), so the resultant either vanishes everywhere or has degree exactly \(mn\).
\end{proof}
